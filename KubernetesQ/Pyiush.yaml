# 1 Check the number of scheduleable nodes excluding tainted and wrte numver to a file:
kubectl get nodes
kubectl describe <node_name>

# 2 Scale the deployment to 4 replicas
kubectl scale deployment <deployment-name> --replicas=4

# 3 Create a network policy that allows access only from the nginx pod in the dev namespace to the redis pod in the test namespace.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-nginx-to-redis
  namespace: test
spec:
  podSelector:
    matchLabels:
      app: redis
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: dev
          podSelector:
            matchLabels:
              app: nginx
      ports:
        - protocol: TCP
          port: 6379

# 4 Perform backup


export ETCDCTL_API=3
export ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt
export ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt
export ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key

etcdctl snapshot save /etc/backup/etcd-snapshot.db

etcdctl snapshot status /etc/backup/etcd-snapshot.db

systemctl stop etcd

etcdctl snapshot restore /var/lib/etcd_bkp/etcd-snapshot.db \\
  --data-dir /var/lib/etcd

systemctl start etcd


# Replace file paths (/etc/kubernetes/pki/etcd/ca.crt, etc.) with actual paths from your cluster setup.

# 5 expose the deployment as nodePort service on port 8080

apiVersion: v1
kind: Service
metadata:
  name: my-deployment-service
  namespace: default
spec:
  type: NodePort
  selector:
    app: my-deployment
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30080

# 6 Monitor the logs of a pod and look for error-not-found and redirect the message to a file

kubectl logs <pod-name> | grep "error-not-found" > /path/to/output-file.log

# 7 Check for the pods that have label env=xyz and redirect the pod name with the highest CPU utilization to a file

kubectl top pods --selector=env=xyz

# 8 Create multi-container pod with image as redis and memcached

apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: default
spec:
  containers:
    - name: redis-container
      image: redis:latest
      ports:
        - containerPort: 6379
    - name: memcached-container
      image: memcached:latest
      ports:
        - containerPort: 11211

# 9 Edit a pod and add initconainer with busybox image and a command

apiVersion: v1
kind: Pod
metadata:
  name: multi-container-pod
  namespace: default
spec:
  initContainers:
    - name: init-container
      image: busybox:latest
      command: ["sh", "-c", "echo Initializing... && sleep 5"]
  containers:
    - name: redis-container
      image: redis:latest
      ports:
        - containerPort: 6379
    - name: memcached-container
      image: memcached:latest
      ports:
        - containerPort: 11211

# 10 Given an unhealthy cluster with a worker node in a Not ready state, fix the clster by SSH into the worker node. Make sure the changes are permanent
sssh into worker node

# to pernament changes:
sudo systemctl daemon reaload

# 11 Create a cluster role, cluster roler binding, and a service accoubnt, cluster role that allows deployment, service, and ds to be created ioinside na namesapce test.

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: test-namespace-role
rules:
  - apiGroups: [""]
    resources: ["deployments", "services", "daemonsets"]
    verbs: ["create", "get", "list", "watch", "update", "delete"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-namespace-sa
  namespace: test
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-namespace-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-namespace-role
subjects:
  - kind: ServiceAccount
    name: test-namespace-sa
    namespace: test


# 12 Make node unschedualable and move the traffic to other nodes (erro ignore daemon sets et.)

kubectl cordon node-1
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data
kubectl get nodes

# To allow the node to accept workloads again, use:

kubectl uncordon <node-name>

# 13 Create a pod and schedule it on node worker01

apiVersion: v1
kind: Pod
metadata:
  name: scheduled-pod
  namespace: default
spec:
  containers:
    - name: nginx-container
      image: nginx:latest
  nodeName: worker01

# 14 Create an ingress resource task in the Day 33 folder

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /service-a
        pathType: Prefix
        backend:
          service:
            name: service-a
            port:
              number: 80
      - path: /service-b
        pathType: Prefix
        backend:
          service:
            name: service-b
            port:
              number: 80

# 15 Create a pv with 1Gi capacity and mode readWriteOnce and no storage class; create a pvc with 500Mi storage and mode as readWriteOnce; it should be bounded with the pv. Create a pod that utilizes this pvc and use a mount path /data

apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /mnt/data

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: example-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  volumeName: example-pv

apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: app-container
    image: nginx
    volumeMounts:
    - mountPath: /data
      name: data-volume
  volumes:
  - name: data-volume
    persistentVolumeClaim:
      claimName: example-
      
#/var/log/pods
#/var/log/containers
#crictl ps + crictl logs
#docker ps + docker logs (in case when Docker is used)
#kubelet logs: /var/log/syslog or journalctl

